# Model Attribution & Tracking

ContextPilot now tracks which AI model generated each message response, providing transparency and better debugging capabilities.

## Features

### Message-Level Model Tracking
- Each assistant message stores the specific model that generated it
- User messages and system messages do not track model information (as they're not AI-generated)
- Model information persists in conversation history

### UI Display
- Assistant messages show a small model badge (e.g., "gpt-4", "claude-3-sonnet-20240229", "llama3.2:latest")
- Model badges are styled with subtle background and monospace font for clarity
- Hover tooltip shows "Generated by [model name]"

### Database Schema
- New `model` column in the `messages` table (nullable for backward compatibility)
- Automatic migration available for existing installations
- Conversation-level model tracking preserved (for conversation metadata)

## Database Migration

### For New Installations
No action needed - the model column is automatically created during database initialization.

### For Existing Installations
Run the migration script to add the model column:

```bash
# Dry run to see what would be done
python3 backend/migration_add_message_model.py --dry-run

# Apply the migration
python3 backend/migration_add_message_model.py
```

The migration:
- Safely adds the `model` column to existing `messages` table
- Uses `NULL` for existing messages (backward compatible)
- Includes rollback protection and error handling

## API Changes

### Message Response Format
Messages now include an optional `model` field:

```json
{
  "id": "msg-123",
  "role": "assistant",
  "content": "Hello! How can I help you?",
  "created_at": "2026-01-16T08:00:00Z",
  "tokens": 25,
  "finish_reason": "stop",
  "model": "gpt-4"
}
```

### Conversation Response Format
Conversations include model info in both conversation metadata and individual messages:

```json
{
  "id": "conv-123",
  "task": "Help with coding",
  "provider": "openai",
  "model": "gpt-4",
  "messages": [
    {
      "role": "user",
      "content": "Hello",
      "model": null
    },
    {
      "role": "assistant", 
      "content": "Hi there!",
      "model": "gpt-4"
    }
  ]
}
```

## Implementation Details

### Backend Changes
- `MessageDB` model updated with `model` column
- `_save_messages()` method enhanced to track model for assistant responses only
- `get_conversation()` API returns model info in message objects
- All AI provider methods (`_generate_openai`, `_generate_anthropic`, `_generate_ollama`) pass model info
- **Model switching support**: Conversation objects update their model field when a different model is requested mid-conversation (v1.1.0+)

### Frontend Changes
- `ConversationMessage` TypeScript interface includes optional `model` field
- Message rendering updated to display model badges
- CSS styling for model badges with provider-specific themes

### Testing
- Comprehensive test suite covering model tracking functionality
- Migration script tests with temporary databases
- Integration tests for all AI providers
- Frontend build validation

## Model Switching in Conversations

ContextPilot supports switching AI models mid-conversation. When you specify a different model for a request in an existing conversation:

1. **The new model is used**: The API call uses the newly specified model
2. **Attribution is accurate**: The response correctly shows which model was actually used
3. **History is preserved**: Each message tracks its generating model independently
4. **Conversation continues**: The conversation context is maintained across model switches

### Example: Switching Models

```python
# Start conversation with GPT-4
response1 = requests.post('/ai/chat', json={
    "task": "Explain quantum computing",
    "model": "gpt-4"
})
conversation_id = response1.json()['conversation_id']

# Continue with GPT-3.5-turbo
response2 = requests.post('/ai/chat', json={
    "task": "Give me a simpler explanation",
    "model": "gpt-3.5-turbo",
    "conversation_id": conversation_id
})

# The second response will correctly show model: "gpt-3.5-turbo"
```

This enables:
- **Cost optimization**: Use expensive models for complex tasks, cheaper ones for simple follow-ups
- **Comparison**: Try different models on the same conversation
- **Flexibility**: Choose the best model for each specific request

## Benefits

1. **Transparency**: Users can see exactly which model generated each response
2. **Debugging**: Easier to identify model-specific issues or behaviors
3. **History**: Complete audit trail of which models were used when
4. **Experimentation**: Compare responses from different models in same conversation
5. **Cost Tracking**: Better understanding of which models are being used most
6. **Flexible Model Selection**: Switch models mid-conversation based on task complexity

## Model Display Examples

### In Chat Interface
```
ðŸ¤– [Assistant Response]
    "Here's the solution to your problem..."
    
    [2:34 PM] [gpt-4] [ðŸ“‹]
```

### Different Providers
- **OpenAI**: `gpt-4`, `gpt-4-turbo`, `gpt-5-preview`
- **Anthropic**: `claude-3-5-sonnet-20241022`, `claude-3-haiku-20240307`
- **Ollama**: `llama3.2:latest`, `mistral:7b`, `codellama:13b`

## Migration Safety

The migration is designed to be safe:
- âœ… Non-destructive (only adds column, doesn't modify existing data)
- âœ… Backward compatible (uses NULL for existing messages)
- âœ… Dry-run mode available for testing
- âœ… Automatic rollback on errors
- âœ… Comprehensive error handling and logging

## Troubleshooting

### Migration Issues
If migration fails:
1. Check database permissions
2. Ensure no active connections during migration
3. Run with `--dry-run` first to test
4. Check logs for specific error messages

### Missing Model Information
If model info doesn't appear:
1. Ensure frontend has been rebuilt after updates
2. Check that API responses include model field
3. Verify database migration completed successfully
4. Check browser console for JavaScript errors

### Display Issues
If model badges don't display correctly:
1. Clear browser cache
2. Ensure CSS updates are loaded
3. Check for TypeScript compilation errors
4. Verify message data includes model field